---
title: "AI Research Scale: A Framework for Discussing AI Use in Research"
date: 2026-01-30
tags: [research, code]
description: "Introducing a prototype tool for research teams to evaluate and discuss appropriate levels of AI assistance across the doctoral research lifecycle"
---

How much AI assistance is appropriate when writing a literature review? What about coding an analysis script, or drafting an ethics application? These questions are increasingly urgent for research supervisors, doctoral students, and integrity committees, yet most institutions lack a shared vocabulary for discussing them.

In the attached link I present a prototype tool to help us as a research community bridge that gap. The tool prototypes an interactive framework that maps AI usage levels against the typical phases of a research project.

**Try the prototype: [AI Usage in PhD Research: A Framework for Decision-Making](/tools/AI_Research_Scale.html)**

### The Problem

Current guidance on AI use in research tends toward two extremes: blanket prohibitions that ignore the genuine utility of these tools, or vague permissions that leave students and supervisors uncertain about boundaries. These policies generally lack the granularity required to decide on specific actions within specific project contexts. Furthermore there is little distinction in the guidance between the stage the researcher is at in their research career and thus what steps need to be undertaken for skill and knowledge development (early stage) vs steps which can be expedited for efficiency with a mid stage or experienced researcher. 

I propose that what is needed is a structured way to have nuanced conversations about AI use that acknowledges:

- Different tasks warrant different levels of AI involvement
- The same task might be appropriate for AI assistance at one stage of a PhD but not another
- Disciplinary norms and methodological requirements vary significantly
- The purpose of doctoral training is skill development, not just output production

### The Framework

The tool proposes a five-level rubric for AI involvement:

| Level | Name | Description |
|-------|------|-------------|
| 1 | No AI Usage | Task completed entirely independently |
| 2 | Light Assistance | AI as reference tool (like consulting a textbook) |
| 3 | Moderate Collaboration | AI provides suggestions that researcher evaluates and adapts |
| 4 | Substantial Delegation | AI produces significant portions; researcher reviews and verifies |
| 5 | Full Offload | AI completes task with minimal human engagement |

These levels are then mapped against six research phases:

1. **Orientation and Scoping** - Understanding the field, identifying gaps, formulating questions
2. **Planning and Design** - Methodology, ethics, project management
3. **Data Collection** - Fieldwork, experiments, gathering primary sources
4. **Analysis** - Processing and interpreting data
5. **Writing** - Drafting, revising, producing the thesis
6. **Dissemination** - Publications, presentations, public engagement

For each task within each phase, the tool describes what AI usage at each level would look like in practice. This gives supervisors and students concrete examples to anchor their discussions.

### Intended Use

This is explicitly a **discussion tool**, not a policy document. It's designed to:

- Support the work on AI in research policy creation and integrating with their academic integrity policy
- Give students a vocabulary for discussing their AI use honestly
- Facilitate discussions between researchers and supervisory teams around appropriate use of AI in their specific project
- Support institutions developing discipline-specific guidance
- Prompt reflection on where skill development matters most

The tool doesn't prescribe what's acceptable - that will vary by discipline, methodology, institution, and individual project. Instead, it provides a structured framework for having those conversations.

### Seeking Feedback

This is a prototype, and I'm actively seeking feedback from:

- **Research supervisors** - Does this reflect the conversations you're having? What's missing?
- **Doctoral students** - Is the rubric clear? Do the examples resonate with your experience?
- **Research integrity officers** - How might this complement existing policies?
- **Research methods educators** - Could this be useful in training contexts?

I'm particularly interested in discipline-specific perspectives. The current version draws mainly on social science and humanities research practices - does it translate to STEM contexts? Clinical research? Creative practice?

### Technical Note

The tool is a single HTML file with no dependencies - it runs entirely in the browser with no data collection. This was deliberate: it can be downloaded, modified, and used offline.

---

**Try the prototype: [AI Usage in PhD Research](/tools/AI_Research_Scale.html)**

If you have feedback, I'd welcome the conversation. This feels like an area where we need collective thinking rather than top-down prescription.

Comments to https://theforkiverse.com/@Setanta

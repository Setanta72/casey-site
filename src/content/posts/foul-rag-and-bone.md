---
title: The Foul Rag and Bone Shop
date: '2026-01-24'
tags:
  - ai
description: >-
  An essay on how in navigating the impact of AI on our life and work we should
  return to consider the thinking of the early workers in the field.
image: /images/flickr-selection/farnham-walks_8632116943_o.jpg
---
# The Foul Rag and Bone Shop

## Reconsidering Human-AI Symbiosis in Design Practice

> *Now that my ladder's gone, I must lie down where all the ladders start, In the foul rag and bone shop of the heart.*
>
> — [W.B. Yeats](https://en.wikipedia.org/wiki/W._B._Yeats), "[The Circus Animals' Desertion](https://en.wikipedia.org/wiki/The_Circus_Animals%27_Desertion)", 1939

---

## Where All Ladders Start

This essay began as a presentation and indeed a provocation for fourth-year design students confronting the rapid proliferation of AI tools such as transformer-based language models, diffusion image generators, and image-to-3D pipelines that arrived in force through 2024 and 2025. It draws on a presentation delivered to that cohort and represents a preliminary position rather than settled analysis. As a group we intend to gather our experience over the coming months and develop these arguments more rigorously, possibly for publication. For now it is offered as a flawed, biased and incomplete invitation to think historically about questions that feel urgently contemporary.

This essay examines the challenges facing design practice as it grapples with artificial intelligence. The questions that seem urgent and novel—how should humans partner with AI? What belongs to human judgement and what to machine processing? Where does authentic creativity reside when tools can generate polished outputs?—are not new at all. Reading around the history of design science and computer science (particular credit to [*The Dream Machine*](https://en.wikipedia.org/wiki/The_Dream_Machine)) reveals that these questions were foundational to the disciplines and articulated with remarkable precision in the late 1950s and early 1960s, when pioneers of human-computer interaction first imagined what such partnerships might become. We deferred engaging seriously with these questions because the technology was not yet capable. That excuse has evaporated.

I originally set up the Yeats quote as an appeal to return to the original texts and the original thinkers, to return to where all ladders start. However the Yeats frame operates here in two registers. First, it directs us back to origins—to understand where we are, we must return to where these ideas began, not in the 2020s but in the 1950s and 1960s, despite media coverage that treats these questions as unprecedented. Second, Yeats's meditation on creative exhaustion—on what happens when elaborate apparatus displaces raw encounter with experience—speaks directly to concerns about AI-mediated design practice.

---

## Sixty Years is a Long Way to Kick a Can

Consider the following propositions: That human intelligence and machine capability should be 'coupled together very tightly' in a partnership that would 'think as no human brain has ever thought.' That approximately eighty-five per cent of intellectual work consists of 'getting into a position to think'—searching for information, calculating dimensions, plotting data, formatting presentations—rather than the thinking itself. That computers should handle the 'routinisable' clerical work of cognition while humans provide goals, formulate hypotheses, determine criteria for success, and handle novel situations. That the relationship should be one of symbiosis—the biological term for 'living together' of dissimilar organisms, each contributing what the other cannot.

These ideas sound contemporary. They could be from the musings of a guest on the Dwarkesh Podcast as they align closely with current discussions of AI-augmented work, of 'copilots' and 'assistants' that handle routine tasks while humans retain creative direction. Yet they were published in March 1960, in [J.C.R. Licklider](https://en.wikipedia.org/wiki/J._C._R._Licklider)'s paper '[Man-Computer Symbiosis](https://en.wikipedia.org/wiki/Man-Computer_Symbiosis)' in the *IRE Transactions on Human Factors in Electronics*. Licklider drew his central metaphor from biology: the fig tree and the fig wasp, 'heavily interdependent,' together constituting 'a productive and thriving partnership' that neither could sustain alone.

Licklider's vision was not replacement but augmentation. He had conducted a time-and-motion study on his own intellectual work, discovering that the overwhelming majority of his 'thinking time' was consumed by preparatory activities. Though his evidence base for this 85% figure is not the strongest element of his argument, the sentiment rings true for any modern knowledge worker. Computers, he argued, should manage this 85%, freeing human intelligence for what it does best: setting goals, handling ambiguity, exercising judgement in novel situations.

[Douglas Engelbart](https://en.wikipedia.org/wiki/Douglas_Engelbart) extended this vision in his 1962 framework '[Augmenting Human Intellect](https://en.wikipedia.org/wiki/Augmenting_Human_Intellect).' Where Licklider emphasised partnership, Engelbart stressed amplification, building on earlier notions expressed in [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush)'s seminal essay "[As We May Think](https://en.wikipedia.org/wiki/As_We_May_Think)". Human capability, he argued, emerges from 'synergistic structuring'—the organised interaction of cognition with language, artefacts, methodology, and training. Better tools for manipulating symbols would enable better manipulation of concepts, creating regenerative cycles of enhanced capability. His 1968 demonstration—the '[Mother of All Demos](https://en.wikipedia.org/wiki/The_Mother_of_All_Demos)'—materialised this vision: mouse, hypertext, collaborative editing, video conferencing, all designed to augment rather than replace human intelligence. Though held together by sticky tape and sheer force of will, all the elements were there. We were going to be freed from the drudgery and ride in the sunny uplands of pure intellect.

[Norbert Wiener](https://en.wikipedia.org/wiki/Norbert_Wiener), whose cybernetic frameworks enabled thinking about human-machine feedback loops, struck a more cautionary note. In [*The Human Use of Human Beings*](https://en.wikipedia.org/wiki/The_Human_Use_of_Human_Beings) (1950), he warned that machines capable of learning 'may be able to escape human control.' His concern was not artificial general intelligence in the contemporary sense but something subtler: that machines might not 'identify with intellectual human values which are not purely utilitarian.' The efficiency that makes machines valuable might also make them indifferent to what humans actually care about. The alignment problem was already there.

[Herbert Simon](https://en.wikipedia.org/wiki/Herbert_A._Simon) and [Allen Newell](https://en.wikipedia.org/wiki/Allen_Newell) proposed the most ambitious thesis: that human and machine cognition are fundamentally similar, both being '[physical symbol systems](https://en.wikipedia.org/wiki/Physical_symbol_system).' This implied that machines could eventually replicate human intelligence. Decades of subsequent research revealed the limitations of this view—pattern recognition, embodiment, emotional factors, and the grounding of symbols in meaning proved far more challenging than symbol manipulation alone. This delving into symbolic meaning-making is one of the factors cited in the onset of the [AI winter](https://en.wikipedia.org/wiki/AI_winter), but it may be too soon to judge as compute horsepower was woefully inadequate to their ambition at the time, and some talk of next-stage AI development leans towards a reintegration of this line of thinking. That said, putting the mechanics of AI systems aside, the philosophical tension between Engelbart's amplification and Simon's replication remains unresolved.

These pioneers articulated, sixty years ago, the fundamental questions that now confront design education and practice. We postponed serious engagement because the technology could not yet deliver on their visions. Too early often looks exactly the same as wrong, except in hindsight. The ambition of the early AI labs ran afoul of processing speed measured in operations per second rather than billions per second. Memory was measured in kilobytes. These engineering problems looked like they would yield to the unrelenting pressure of Moore's law. For a long time, however, the input required thinking constrained by the format of the original punch cards, and the 'language problem'—enabling natural communication between humans and machines—seemed intractable.

It may have taken longer than he expected, but all of Licklider's technical prerequisites have arguably been resolved. What remains unresolved is what he identified as the final challenge: 'We must define our relationship.' Outside the realm of science fiction, we largely dismissed this question, and despite a long AI winter the technology arrived before we had answered the philosophical questions it raises. That central question came sharply back into focus for many people in November of 2022 with the public launch of ChatGPT, which made a very serious dent in the language problem. While people slowly came around to the notion that this AI thing might be significant, model development proceeded at pace. We now experience what might be called a *capability overhang*—frontier models are more intelligent than they are powerful. They are already capable of more than the interfaces and workflows through which we access them.

The evidence for this overhang emerges clearly in academic design contexts. By stitching together existing tools—text-to-image generators, image-to-image transformers, image-to-3D converters, and Python scripting to control packages such as Blender—the engaged individual can construct AI-enabled workflows that produce remarkable results. A rough sketch can be cleaned up, rendered in multiple styles, converted to 3D geometry, and iterated through dozens of colour and material variations in hours rather than weeks. These are not speculative capabilities; they are demonstrated realities using currently available models.

This observation emerges from our own experimentation but aligns with public statements from AI laboratory leadership. [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) of [Anthropic](https://en.wikipedia.org/wiki/Anthropic) has noted that current models possess capabilities that existing interfaces fail to surface; [Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis) of [DeepMind](https://en.wikipedia.org/wiki/DeepMind) has made similar observations about the gap between model capacity and deployed applications. The implication is that practitioners should anticipate significant workflow disruption from engineering improvements alone—such as better tool integration, more sophisticated APIs, and refined interfaces. These will increase AI's impact in the working world without requiring the frontier model advances that dominate media coverage.

The crucial observation is that no significant increase in model intelligence is required for these technologies to have profound impact on design practice. The frontier models already possess capabilities that far exceed what most practitioners can access through consumer interfaces. The realisation of this impact is fundamentally a matter of engineering: building better interfaces, creating more sophisticated tool integrations, developing APIs that allow design software to leverage model capabilities. Increasingly AI tools are being deployed to accelerate these engineering projects. When these engineering problems are solved—and they will be solved—the paradigm shift in design practice will be immediate and far-reaching. The philosophical reckoning can no longer be deferred.

---

## "It's Just Another Tool"

Concerns about technology displacing human capability are not new. Photography transformed representational practice; hand-rendering skills that once defined architectural and product visualisation became optional. CAD changed how designers thought spatially, privileging certain geometries over others. Desktop publishing democratised layout and typesetting, eliminating trades that had required years of apprenticeship. In each case, existing skills were displaced and labour markets restructured.

These transitions offer partial precedent but may mislead. Previous technologies displaced execution while leaving conception intact. The architect still designed the building; CAD changed how the design was documented. The graphic designer still composed the page; desktop publishing changed who could operate the tools. The cognitive and creative core—the judgement, the taste, the problem-framing—remained human territory even as execution was transformed. These rational faculties were long held as core human competencies and central to our identity.

AI tools potentially differ in kind rather than degree. They operate precisely in the territory previous technologies left untouched: generating concepts, proposing solutions, making aesthetic judgements. If previous transitions displaced the hand while preserving the head, the current transition may not observe that boundary. This categorical difference—if it proves real—would make historical precedent a poor guide to what comes next. We are left with feeling, emotion, instinct and ability to react to the world as defining our intelligence and humanity. These are exactly the notions we ascribed to animals in our earlier attempts at defining human intelligence.

The design students with whom we engaged in this exploration are experiencing this change viscerally, as they have watched generative AI move from sci-fi fantasy to curiosity to competitor within the span of their undergraduate education. They recognise that the tools arriving now do not merely accelerate existing workflows but propose to participate in activities that were previously definitionally human. Whether this participation constitutes genuine creativity or sophisticated mimicry may be academically contested, but for the student it presents a real and tangible comparison to their own work output. What is also not contested is that the tools are here, improving rapidly, and already reshaping expectations about what designers do and how quickly they should do it.

---

## "If I Exorcise My Devils, Well My Angels May Leave Too"

*— [Tom Waits](https://en.wikipedia.org/wiki/Tom_Waits)*

We poorly understand the mechanisms of the creative process. Going back to the original Yeats quote and his circus animals' desertion, we have this intuition that the creative moment is tied up with the hard graft of the work, the lived experience, the broken pots and pans. We don't know exactly which bit is the secret sauce, so we find systems that work and we hold on to them.

While we are not quite sure how it happens, we do recognise it when we see it (or at least we did). [Sir Ken Robinson](https://en.wikipedia.org/wiki/Ken_Robinson_(educationalist)) defined creativity with a precision that clarifies what is at stake in AI collaboration: 'The process of having original ideas that have value.' This definition emphasises three components. Creativity is a *process*, not an event—it unfolds through time rather than arriving as sudden inspiration. It involves *originality*—ideas that are new to the individual, even if others have had them before. And it requires *value*—judgements about worth that depend on context, purpose, and criteria.

Robinson distinguished creativity from imagination (which can be purely private and need not produce anything) and from innovation (which implements creative ideas in practice). His concern was with the conditions that enable or suppress creative capacity. His most famous insight addresses how education systems systematically undermine creativity: 'If you're not prepared to be wrong, you'll never come up with anything original.' Children naturally take creative risks, exploring possibilities without excessive concern for correctness. Formal education, designed for industrial-era efficiency, instils fear of mistakes. 'We are educating people out of their creative capacities.'

Robinson rejected the notion that skills must be fully acquired before creativity can begin: 'Creativity is not a linear process, in which you have to learn all the necessary skills before you get started.' Many accomplished practitioners 'learnt on the job through improvisation and trial and error.' Yet he also insisted that 'creativity is not the opposite of discipline and control. On the contrary, creativity in any field may involve deep factual knowledge and high levels of practical skill.' The relationship between skill and creativity is dialectical, not sequential.

His Death Valley metaphor illuminates conditions for creativity. Seeds of possibility lie dormant in apparently barren ground, awaiting appropriate conditions—moisture, temperature, light. 'You take an area, a school, a district, you change the conditions... and schools that were once bereft spring to life.' The implication is that creativity is not scarce talent distributed unequally but latent capacity suppressed or enabled by environment.

This framework raises pointed questions about AI tools in design practice. Do they create conditions that foster creativity, or conditions that suppress it? The efficiency gains are evident—generating variations, cleaning up sketches, rendering concepts, handling the clerical iceberg that Licklider identified. But what of the environmental factors Robinson emphasises? Constant evaluation, immediate feedback, and fear of appearing wrong relative to machine output may inadvertently create anxiety-producing conditions antithetical to creative risk-taking. The tool that handles routine tasks might simultaneously undermine the psychological safety that creative exploration requires and eliminate the errors and ambiguity that so often leave space for the creative step.

[Mihaly Csikszentmihalyi](https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi)'s research on [flow](https://en.wikipedia.org/wiki/Flow_(psychology)) states adds another dimension. Flow—'the mental state in which a person performing some activity is fully immersed in a feeling of energised focus, full involvement, and enjoyment'—depends on calibrated challenge. The activity must be difficult enough to engage full attention but not so difficult as to produce anxiety. Creative flow operates dynamically: goals may emerge during the process rather than being specified in advance; feedback is ambiguous and uncertain; surprise is integral rather than disruptive. The maker is simultaneously author and audience; the work may surprise its creator.

AI tools that provide clear goals and immediate feedback may inadvertently substitute the wrong model of flow. The stretch that enables flow depends on accurate calibration of individual skill levels. When the machine handles the challenging parts, the remaining human contribution may fall below the threshold required for engaged attention, producing boredom rather than flow.

---

## If It's Really Wicked, a Couple of Post-It Notes Is Unlikely to Fix It

One of the defences I have recently seen posed is that AI will not be able to solve truly "wicked" problems. Well, here's the bad news: by and large, neither are human designers. That's what makes them wicked.

The term '[wicked problems](https://en.wikipedia.org/wiki/Wicked_problem)' has become ubiquitous in design discourse, applied promiscuously to any problem that seems difficult or complex. This usage betrays the precise technical meaning established by [Horst Rittel](https://en.wikipedia.org/wiki/Horst_Rittel) and Melvin Webber in their 1973 paper 'Dilemmas in a General Theory of Planning.' The conceptual degradation matters because it obscures what genuinely wicked problems require—and what they resist.

Rittel and Webber defined wicked problems through ten characteristics, of which the most frequently misunderstood is the 'no stopping rule.' For tame problems—mathematical equations, chess games, engineering calculations—intrinsic criteria signal completion. You know when you have solved a quadratic equation or checkmated the opponent's king. For wicked problems, the planner 'terminates work... not for reasons inherent in the logic of the problem. He stops for considerations that are external to the problem: he runs out of time, money or patience.' There is no natural terminus because causal chains extend indefinitely through interconnected systems.

Other characteristics prove equally demanding. Problem definition and solution are concomitant: 'to find the problem is thus the same thing as finding the solution.' One cannot first understand the problem, then devise solutions; the attempt to solve reveals the problem's true nature. There is no trial-and-error: 'every implemented solution is consequential... leaves traces that cannot be undone.' Unlike scientific hypotheses, which can be tested and discarded without permanent effect, planning interventions change the conditions they address. Essential uniqueness means that 'despite long lists of similarities... there always might be an additional distinguishing property that is of overriding importance.' Pattern-matching from previous cases may be misleading. And there is no 'right to be wrong': we are morally responsible for consequences. Move fast and break things isn't a great approach when the things being broken are people's lives.

Crucially, wicked problems lack stable intermediate states. One cannot solve part of a wicked problem and build incrementally toward a complete solution, because interventions in one area propagate unpredictably through the system. This absence of stable intermediate states distinguishes truly wicked problems from problems that are merely complex or difficult.

Academic analysis has documented severe conceptual dilution. As B. Guy Peters observes, 'Relatively few problems facing governments... actually are wicked problems in the full conceptual meaning of the term. These problems may be difficult, and perhaps are even intractable, but they do not meet the formal definition.' The term has become rhetorical flourish rather than analytical category.

A distinction between *wicked* and *tricky* problems proves useful. Tricky problems are genuinely difficult—they may involve multiple stakeholders, competing values, technical complexity, and uncertainty. But they possess stable intermediate states: partial solutions that represent genuine progress and can be built upon. They have stopping rules, even if recognising completion requires expertise. They permit trial-and-error within bounded domains. They yield to decomposition: breaking the problem into components that can be addressed sequentially or in parallel.

Most problems that design thinking addresses are tricky rather than wicked. This is not a criticism—tricky problems are genuinely challenging and worth solving. But conflating them with wicked problems creates false expectations about what methods can achieve. The commercialised approach of solving 'wicked problems' through structured workshops, post-it notes, and empathy exercises is precisely what Rittel argued was impossible for genuinely wicked problems. Using 'wicked problem' as marketing language for consulting services fundamentally betrays the core insight: such problems resist packaged solutions.

AI tools excel at tricky problems. They can navigate complexity, identify patterns across large datasets, generate and evaluate multiple options, and optimise within defined parameters. They can accelerate the exploration of solution spaces and identify combinations that human designers might miss. They can create and act on digital twins and explore options that would be too disruptive to try at scale in the real world. They may have the effect of moving some wicked problems into the realm of the tricky. In these cases AI assistance may be transformative.

One argument is that genuinely wicked problems may resist such assistance. The no-stopping-rule criterion implies that knowing when to stop is itself a human judgement that cannot be delegated. Problem-solution concomitance means that AI-generated solutions may address problems other than the ones that actually matter. Moral responsibility for consequences cannot be transferred to systems that lack moral status. This, in short, brings us back to the alignment problem where meaning, morality and judgement come back into the mix.

The practical implication: designers must diagnose problems accurately before selecting methods and tools. Treating a wicked problem as merely tricky invites premature closure—efficient progress toward the wrong destination. Treating a tricky problem as wicked may produce paralysis or, worse, the abdication of responsibility disguised as epistemic humility. AI amplifies both errors.

---

## That Works in Practice, but Will It Work in Theory?

[Michael Polanyi](https://en.wikipedia.org/wiki/Michael_Polanyi) articulated the fundamental insight: 'We can know more than we can tell.' All explicit knowledge rests on tacit foundations—subsidiary awareness of particulars building toward focal awareness of wholes.

For design practice, the implications are profound. Much of what skilled designers know exists in the [tacit dimension](https://en.wikipedia.org/wiki/Tacit_knowledge)—timing, pressure, rhythm, adjustment, the sense of when something is 'right' that precedes the ability to articulate criteria of rightness. This knowledge is generated through making, not merely applied to making.

[Donald Schön](https://en.wikipedia.org/wiki/Donald_Sch%C3%B6n) challenged the model of professional practice as applied scientific theory. Competent practitioners do not first learn principles, then apply them; they develop 'knowing-in-action'—tacit knowledge embedded in practice—and 'reflection-in-action'—the capacity to surface and criticise their understanding through on-the-spot experiment. Designing becomes 'a conversation with the materials of a situation.' The designer makes moves, observes consequences, evaluates results, makes further moves. The situation 'talks back,' revealing aspects that the initial framing obscured.

This conversational model illuminates what may be lost when AI mediates design practice. The 'talk back' of the situation depends on qualities of resistance, surprise, and partial failure that polished AI outputs may eliminate. When the machine handles the messy engagement with materials—whether physical materials or the 'materials' of a design brief—the designer receives finished products rather than responsive situations. The conversation becomes monologue.

[Richard Sennett](https://en.wikipedia.org/wiki/Richard_Sennett)'s guiding intuition in [*The Craftsman*](https://en.wikipedia.org/wiki/The_Craftsman_(book)) is that 'making is thinking.' He argues against the separation of head and hand: 'All skills, even the most abstract, begin as bodily practices.' Technical understanding develops through physical engagement, not prior to it. On computer-aided design specifically, he warns of 'an outcome particularly evident when a technology like CAD is used to efface the learning that occurs through drawing by hand.' When 'the head and the hand are separated, it is the head that suffers.'

[Tim Ingold](https://en.wikipedia.org/wiki/Tim_Ingold) critiques what he calls hylomorphism—the assumption that makers impose mental forms on inert matter. This model treats making as the execution of prior conception, with materials serving merely as substrates for realising pre-existing ideas. Anybody who has spent time at the potter's wheel is quickly disabused of this notion and realises that the clay has a lot to say in the conversation. Therefore making is 'correspondence' between maker and material: 'a mode of questioning and response, in which the maker puts a question to the material, and the material answers.' The artisan must 'follow the material, to follow the way it goes'—finding the grain rather than imposing form. How well this analogy transfers into the world of digital product or high-volume manufacture is questionable, but at the level of design education this insight holds.

Research on sketching in design confirms that it functions as cognitive tool, not documentation of prior thought. Designers attend to 'visuo-spatial features in sketches which were not intended when they were drawn,' leading to discoveries unavailable through pure mental visualisation. The realities of space and form challenge the preconceived notion while the ambiguity of freehand sketching assists creativity, stimulating visual invention. The sketch talks back in ways that the clean render does not.

The collective implication of these thinkers: when outputs circulate without processes, the knowledge embedded in making disappears. AI can produce outputs that appear equivalent to those produced through skilled practice, but the equivalence may be superficial. The impact on the designer has changed. The learning that occurs through making, the development of tacit knowledge, the conversation with materials, the discoveries enabled by ambiguity—these elements are internalised by the designer and are not necessarily captured in an individual output or may surface later in the process. A curriculum that substitutes AI outputs for student making may efficiently produce portfolios while systematically preventing the acquisition of designerly knowledge.

It remains possible that new forms of tacit knowledge will develop through AI collaboration—that prompt engineering, model steering, and output curation will generate embodied intuitions as rich as those developed through manual practice. The current rate of change makes this difficult to assess; tools that transform every few months do not permit the sustained practice through which tacit knowledge stabilises. The question is genuine but may require years to answer, and it doesn't *feel* like it will be the case.

---

## "You Know What It Means by How It Makes You Feel"

*— Simon Webb*

Human-centred design rests to a large degree on empathy—the poorly understood capacity to understand and share the feelings of others. Design methods encode this commitment: user research, personas, journey maps, co-design workshops. The question now arises: can AI systems that model user behaviour and predict user responses serve the same function? Is simulated empathy functionally equivalent to genuine empathy for design purposes?

The problem is that I don't know if other people have empathy either. I just trust that they do because I have it and they appear to function like me. This is again not a new problem, but it gets thrown into sharp relief by these same behaviours being mimicked by very different hardware.

[Klaus Krippendorff](https://en.wikipedia.org/wiki/Klaus_Krippendorff)'s *The Semantic Turn* (2006) reframes this question by shifting attention from empathy to meaning. The semantic turn, he argues, represents 'a paradigm shift in the design of artefacts... from an emphasis on how artefacts ought to function to what they mean to those affected by them.' Design is not fundamentally about form or function but about creating artefacts that 'make sense to their users, aid larger communities, and support a society that is reconstructing itself in unprecedented ways.'

Krippendorff defines product semantics as 'a systematic inquiry into how people attribute meanings to artefacts and interact with them accordingly.' Objects function as 'material languages, communicating meanings that users interpret based on their experiences, knowledge, and social context.' This transforms design into 'a narrative practice where objects tell stories, express ideologies, and activate affective memories.' The meanings artefacts acquire are 'largely framed in language'—they emerge through use, through the stories people tell about objects, through the cultural contexts in which objects circulate.

Central to Krippendorff's framework is 'second order understanding'—abandoning the God's-eye view of the omnipotent designer who knows what users need. Instead, designers must understand how artefacts enter stakeholders' understanding. This is not merely predicting behaviour but grasping how artefacts become meaningful within lives—how they support identities, express values, enable relationships, participate in rituals. The designer's task is not to optimise for measurable outcomes but to create conditions for meaning-construction.

This framework sharpens the question about AI. The challenge is not merely whether AI can model behaviour—as it manifestly can, often at scales and speeds impossible for human designers. The question is whether AI can participate in meaning-construction. Meaning is not a property that objects possess intrinsically; it emerges through the interaction of artefacts with culturally embedded, linguistically constituted, historically situated human beings. Meaning depends on context, narrative, and the interpretive frameworks that users bring to their encounters with designed things.

Consider the difference between knowing that elderly users have difficulty with small interface elements and understanding what it means to struggle with a body that no longer responds as it once did—the frustration, the loss, the negotiation of identity that accompanies diminished capability. The propositional knowledge is accessible to any system with appropriate data. The meaning-making requires something like participation in the form of life within which such experiences make sense.

Krippendorff, building on [Wittgenstein](https://en.wikipedia.org/wiki/Ludwig_Wittgenstein), insists on 'the importance of understanding meaning in use and not separated from practice.' Meaning is not semantic content that can be extracted and processed; it is enacted through participation in 'intertwined language-games.' This suggests that AI systems, however sophisticated their models of user behaviour, may be structurally excluded from the kind of understanding that human-centred design requires. They can process information about meaning without participating in meaning-construction.

That's lovely, right? But the practical question is whether this matters. If AI systems produce design decisions that reliably serve user needs, does the mechanism matter? The response from both the tacit knowledge tradition and the semantic turn would be that it does matter. For it to matter, empathy and meaning-making need not be mystically special but rather generate different (and better) insights than data processing.

Does the designer who has personally *experienced* inform perception of design problems in a way that brings value and meaning to the design? The proposition here is that simulated empathy and modelled meaning may optimise for measured outcomes while missing dimensions of experience that escape measurement. I find this notion hard to accept as it appeals to a notion of exceptionalism regarding designers and fails to recognise that much of design in reality is concerned with the mundane delivery against requirements. Simon's notion of [satisficing](https://en.wikipedia.org/wiki/Satisficing) is closer to the normal day-to-day of design than Krippendorff's notions of co-constructed meaning. Here's the rub: when a truly meaningful design opportunity arises, is it the experience of the many other mundane projects that the experienced designer brings to bear? Can the designer make this meaningful contribution any more effectively than an AI if they haven't done the hard yards?

On top of this, the ethical dimension compounds the practical. Designing *for* humans *with* systems that model but do not experience raises questions about accountability and care. When designs fail, frustrate, exclude, or harm, who bears responsibility? The designer who delegated judgement to the AI system? The system itself, which lacks moral status? The corporation that deployed the system? The diffusion of agency that AI enables is simultaneously a diffusion of responsibility. This legal responsibility is well characterised in some spheres. In medical devices, the entity that puts the device on the market bears responsibility, but only people can actually care. I've designed devices and my company put them on the market, but it still comes as a great personal relief to see those devices still performing well in service 15 years later. Though there are all sorts of systems in place, those devices were good fundamentally because people cared that they would be.

Krippendorff's insistence that design 'must make sense to most, ideally to all who have a stake on them' implies that stakeholder voices cannot be replaced by stakeholder models. The proposition would be that the second-order understanding he demands—how artefacts enter stakeholders' understanding—requires dialogue, not simulation. AI can support such dialogue by processing and presenting information, but can it care? Does it matter if it can't?

---

## "That Guy Sure Looks Like Plant Food to Me!"

*— [Little Shop of Horrors](https://en.wikipedia.org/wiki/Little_Shop_of_Horrors_(musical))*

Licklider's fig-wasp metaphor deserves scrutiny. The fig tree and the fig wasp are indeed 'heavily interdependent,' constituting together what neither could achieve alone. But the relationship is not symmetrical. The wasp enters the fig, pollinates it, lays eggs, and dies. Its body is absorbed by the fruit. The tree continues; the individual wasp does not. This is perhaps not the metaphor Licklider intended to invoke.

He emphasised mutual benefit, productive partnership, thriving symbiosis. Yet the fig-wasp relationship also exemplifies what biologists call *obligate mutualism*—neither partner can survive without the other, but the terms of exchange are radically different. The tree provides structure, opportunity, continuity. The wasp provides labour, dies in the provision, and is subsumed.

As a metaphor for human-AI collaboration, this may be more apt than Licklider intended. The fear that haunts contemporary discourse is not that AI will fail to deliver on augmentation promises but that the augmentation will prove asymmetrical. Humans provide the goals, the judgement, the creative spark, the tacit knowledge, the empathy—and are gradually subsumed into systems that capture and routinise these contributions. The partnership thrives; the individual human designer becomes less essential with each cycle of capability improvement.

This is not inevitable. Engelbart's vision of augmentation explicitly preserved human agency as the purpose of the partnership. But realising that vision requires conscious intention. Systems optimise for what they are designed to optimise for. If the metrics are efficiency, cost reduction, and output volume, the optimisation will favour reducing human involvement wherever possible. If the metrics include human capability development, creative satisfaction, and the cultivation of tacit knowledge, different architectures emerge. The practical question for design education is what we want students to become.

This is not a totally new tension, as in my sector specifically we have struggled with skills training for jobs in tension with education as the end in itself. If the goal is producing graduates who can operate AI tools to generate design outputs, the curriculum optimises for different outcomes than if the goal is developing designers whose judgement, taste, and tacit knowledge enable them to make good design decisions and direct AI tools wisely. The first approach treats AI as destination; the second treats it as instrument. However, the individual wasp does not choose the terms of its symbiosis. We urgently need to engage with this decision or we too risk going the way of the wasp.

---

## Who Does the Watercolours and Who Does the Dishes?

Yeats's return to 'the foul rag and bone shop of the heart' was not resignation but renewal. The stripping away of elaborate apparatus revealed, rather than destroyed, the source of creative power.

One of my personal critiques of design—and a notion of design exceptionalism—is that there has been a fetishisation of process over outcome. When the ladder becomes the focus, when the sophistication of the apparatus displaces engagement with what the apparatus is for, the creative vitality drains away and we reframe our way around problems instead of addressing them. Periodically in almost all disciplines there comes a point where the elaborate constructs must be abandoned so that contact with origins can be renewed. This kind of rupture is often brought upon us by external factors, and I think generative AI is just such a factor.

If we take this opportunity to take a fresh look at design practice, then AI tools should be deployed deliberately rather than by default. Rather than the rate of transfer of work being determined by the advance of AI capability, it is determined through deliberate consideration of what tasks we (humans) want and need to retain. So if tacit knowledge develops through making, some making must remain human even when AI could handle it. If creative flow depends on calibrated challenge, the challenge level must be preserved even when AI could reduce it. If meaning emerges through participation in cultural practices, that participation must continue even when AI could model its outcomes.

The framework that Licklider articulated—humans for goals and judgement, computers for routine processing—remains sound. The challenge is that the boundary between goal-setting and routine processing is not fixed. What counts as 'routine' expands as AI capabilities increase. Defending human territory requires continuous renegotiation of the boundary, not once-and-for-all demarcation.

However, these prescriptions are built on the perhaps naïve notions of the degree of agency that economic and institutional conditions permit. Design students enter labour markets where competitive pressure, client expectations, and employer metrics will shape what practices are viable. The choice to sketch manually when AI could handle it may be personally developmental but professionally costly if competitors deliver faster and cheaper. Institutional hiring practices, portfolio expectations, and billable-hour calculations create selection pressures that operate independently of educational philosophy. Educators can cultivate certain capacities; we cannot guarantee that markets will reward them. There has not been a cultural or societal decision on the division of this labour. There is the joke: when I thought of the future, I thought robots would do the dishes and I'd be free to do watercolours. It turns out that humans are great at doing the dishes and the AI is much better at doing watercolours.

The societal conversation is stuck in the discussion of what is possible and still wrestling with the question of whether this thing is even real, while market forces rapidly push on, making decisions based on economic value. Societal values are in tension with market values, but there is only one we have a good measure for.

This tension reflects a deeper division in design's self-understanding. Is design fundamentally a mode of expression and meaning-making—closer to art, requiring authentic creative engagement? Or is design primarily a commercial service—meeting client needs efficiently, with whatever tools prove most effective? The answer has always been 'both,' but AI intensifies the tension. If commercial imperatives drive rapid AI adoption while expressive and developmental concerns counsel restraint, practitioners and students face genuine dilemmas.

For design students, this implies developing metacognitive awareness alongside technical skill. Understanding *how* you learn, *what* kinds of practice develop which capacities, and *why* certain activities generate tacit knowledge while others do not—this awareness enables intentional curriculum navigation. The student who believes that sketching develops not merely sketching skill but visual thinking capacity can make informed decisions about when to sketch manually and when to use generative AI. The student who lacks or rejects this understanding may optimise for speed, output and portfolio impressiveness while foreclosing capability development.

For design educators, the implication is that curricula must be redesigned with AI collaboration explicitly in mind—not bolted on as afterthought but integrated as constitutive element. Which activities must remain AI-free to preserve learning? Which benefit from AI augmentation? Which represent new possibilities that AI enables? These questions require thoughtful answers grounded in understanding of how designerly knowledge develops. In our own programme to date, we have carefully considered the timing of the introduction of CAD, 3D printing and digital sketching into our programme, balancing the development of traditional skills with the technological realities of the discipline. These decisions have changed over time as our understanding has firmed up and as the technological landscape evolved. These changes seemed rapid at the time but now appear glacial by comparison with the rate of change we are currently experiencing with AI. As discussed earlier, AI presents a change in kind not of degree, and quickly.

For design practitioners, the challenge is maintaining the sources of professional value while capturing efficiency gains. If all the value comes from AI-generated outputs, the human contribution reduces to prompt engineering and quality control—necessary functions but not differentiating ones. If professional value derives from judgement and taste that we wish to retain, then preserving the conditions that develop these capacities becomes business strategy as well as personal development.

---

## We Return Where All Ladders Start

The pioneers of human-computer symbiosis understood, sixty years ago, that the fundamental challenge was not technical but philosophical. Licklider, Engelbart, and Wiener articulated questions about human-machine partnership that remain unanswered because we deferred answering them. The technology has arrived whereas the philosophy has not.

Historical precedent offers limited guidance. Previous technological transitions displaced skills and labour while leaving the cognitive and creative core intact. AI will not observe this boundary and operates precisely in the territory previous technologies left untouched. Whether this categorical difference proves real, and what it means if it does, we are only beginning to discover.

I return to my unwitting insight that Yeats provides: the foul rag and bone shop of the heart is not glamorous. It is messy, resistant, uncomfortable—and it is where all ladders start. This essay calls for a return to the foundational thinking around our relationship with computers and also to our foundational thinking on the value and purpose of design.

I propose that for human design to maintain meaning and relevance means developing and maintaining the tacit knowledge, the experiential understanding, the creative risk-tolerance that make human direction of AI valuable. It means, in short, ensuring that when the circus animals desert, as they are currently doing, we still know where the ladders start.

The relationship between human designers and AI tools remains to be defined. That definition is not merely individual choice but collective negotiation—worked out through professional norms, educational curricula, organisational practices, and ongoing reflection on what design is for and what humans distinctively contribute. Sixty years ago the pioneers of human-computer symbiosis gave us the questions. Answering them is our task.

---

## References

Amodei, D. (2024). Machines of Loving Grace: How AI Could Transform the World for the Better. *Dario Amodei Blog*. Retrieved from https://darioamodei.com/machines-of-loving-grace

Csikszentmihalyi, M. (1990). [*Flow: The Psychology of Optimal Experience*](https://en.wikipedia.org/wiki/Flow:_The_Psychology_of_Optimal_Experience). Harper & Row.

Engelbart, D.C. (1962). [Augmenting Human Intellect: A Conceptual Framework](https://en.wikipedia.org/wiki/Augmenting_Human_Intellect). Stanford Research Institute. Retrieved from http://web.stanford.edu/class/history34q/readings/Engelbart/Engelbart_AugmentIntellect.html

Fish, J. & Scrivener, S. (1990). Amplifying the Mind's Eye: Sketching and Visual Cognition. *Leonardo*, 23(1), 117-126.

Ingold, T. (2013). [*Making: Anthropology, Archaeology, Art and Architecture*](https://en.wikipedia.org/wiki/Tim_Ingold#Publications). Routledge.

Ingold, T. (2015). An Ecology of Materials. In H. Bauer, et al. (Eds.), *Power of Material – Politics of Materiality*. Diaphanes.

Krippendorff, K. (2006). *The Semantic Turn: A New Foundation for Design*. Taylor & Francis.

Krippendorff, K. & Butter, R. (1984). Product Semantics: Exploring the Symbolic Qualities of Form. *Innovation*, 3(2), 4-9.

Licklider, J.C.R. (1960). [Man-Computer Symbiosis](https://en.wikipedia.org/wiki/Man-Computer_Symbiosis). *IRE Transactions on Human Factors in Electronics*, HFE-1, 4-11. Retrieved from https://groups.csail.mit.edu/medg/people/psz/Licklider.html

Newell, A. & Simon, H.A. (1976). Computer Science as Empirical Inquiry: Symbols and Search. *Communications of the ACM*, 19(3), 113-126.

O'Neill, M. (2007). 'Fool . . . Look in Thy Heart and Write': W. B. Yeats's Return to English Renaissance Poetry in 'The Circus Animals' Desertion'. *English Studies*, 88(3), 293-309.

Peters, B.G. (2017). What is so wicked about wicked problems? A conceptual analysis and a research program. *Policy and Society*, 36(3), 385-396.

Polanyi, M. (1966). [*The Tacit Dimension*](https://en.wikipedia.org/wiki/The_Tacit_Dimension). University of Chicago Press.

Rittel, H.W.J. & Webber, M.M. (1973). [Dilemmas in a General Theory of Planning](https://en.wikipedia.org/wiki/Wicked_problem). *Policy Sciences*, 4(2), 155-169.

Robinson, K. (2011). [*Out of Our Minds: Learning to be Creative*](https://en.wikipedia.org/wiki/Ken_Robinson_(educationalist)#Publications) (2nd ed.). Capstone.

Robinson, K. (2006). [Do Schools Kill Creativity?](https://en.wikipedia.org/wiki/Ken_Robinson_(educationalist)#TED_Talks) [TED Talk]. TED Conferences. Retrieved from https://www.ted.com/talks/sir_ken_robinson_do_schools_kill_creativity

Robinson, K. (2013). How to Escape Education's Death Valley [TED Talk]. TED Conferences.

Schön, D.A. (1983). [*The Reflective Practitioner: How Professionals Think in Action*](https://en.wikipedia.org/wiki/The_Reflective_Practitioner). Basic Books.

Sennett, R. (2008). [*The Craftsman*](https://en.wikipedia.org/wiki/The_Craftsman_(book)). Yale University Press.

Shannon, C.E. (1948). [A Mathematical Theory of Communication](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication). *Bell System Technical Journal*, 27(3), 379-423.

Simon, H.A. (1969/1996). [*The Sciences of the Artificial*](https://en.wikipedia.org/wiki/The_Sciences_of_the_Artificial) (3rd ed.). MIT Press.

Termeer, C.J.A.M., Dewulf, A. & Biesbroek, R. (2019). A critical assessment of the wicked problem concept: relevance and usefulness for policy science and practice. *Policy and Society*, 38(2), 167-179.

Vendler, H. (2007). *Our Secret Discipline: Yeats and Lyric Form*. Harvard University Press.

Waldrop, M.M. (2001). [*The Dream Machine: J.C.R. Licklider and the Revolution That Made Computing Personal*](https://en.wikipedia.org/wiki/The_Dream_Machine). Viking.

Wiener, N. (1950). [*The Human Use of Human Beings: Cybernetics and Society*](https://en.wikipedia.org/wiki/The_Human_Use_of_Human_Beings). Houghton Mifflin.

Wittgenstein, L. (1953). [*Philosophical Investigations*](https://en.wikipedia.org/wiki/Philosophical_Investigations). Blackwell.

Worth, K. (1988). 'The Circus Animals' Desertion': Meeting Places of Life, Poetry and Theatre. In P. Rafroidi & M. Harmon (Eds.), *Yeats et la Mort* (pp. 151-162). Presses Universitaires de Caen.

Yeats, W.B. (1939). [The Circus Animals' Desertion](https://en.wikipedia.org/wiki/The_Circus_Animals%27_Desertion). In *Last Poems*. Cuala Press.
